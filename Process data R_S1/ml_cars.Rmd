---
title: "machine learning with R"
author: "Ayoub Yousssoufi"
date: "11/29/2021"
output: html_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Task

Applying one supervised and one non-supervised model on a dataset mtcars

```{r echo = TRUE,message=FALSE,echo=FALSE,cache.comments=FALSE}
library(ggplot2)
library(caTools)
library(caret)
library(dplyr)
library(explore)
library(randomForest)
library(fpc)
library(factoextra)
```

## Data processing of the mtcars file

```{r}
data <- read.csv("mtcars.csv")
data %>% explore_tbl()
data %>% describe()
```

```{r}
data_rescale <- data %>% 
  mutate_if(is.numeric, funs(as.numeric(scale(.)))) 
head(data_rescale)
```

<u>**Split the data to training and testing data and print the dimension**</u>

```{r, echo=FALSE}
create_train_test <-
  function(data, size = 0.8, train = TRUE) {
    n_row = nrow(data)
    total_row = size * n_row
    train_sample <-
      1:total_row
    if (train == TRUE) {
      return (data[train_sample, ])
    } else {
      return (data[-train_sample, ])
    }
  }
data_train <- create_train_test(data, 0.8, train = TRUE)
data_test <- create_train_test(data, 0.8, train = FALSE)
```

```{r}
dim(data_train)
dim(data_test)
```

<u>**Model training for linear regression & prediction**</u>

```{r}
model <- lm(mpg ~ disp, data = data_train)
new_data <- dplyr::tibble(disp = data_test$disp)
data_test$output <- predict(model, new_data)
```

<u>**Mean Square Error (MSE)**</u>

```{r}
sqrt(sum(data_test$mpg - data_test$output) ** 2 / nrow(data_test))
```

The scatter plot of the vehicle type depending on the fuel efficiency

```{r,echo=FALSE}
data %>% 
  ggplot(aes(x = mpg, y = am)) +
  geom_point() + theme_bw() +
  xlab("Fuel Efficiency (Miles/Gallon)") +
  ylab("Vehicle Transmission Type (0 = Automatic, 1 = Manual)")
```

<u>**Spliting dataframe to training/testing.**</u>

```{r}
set.seed(10)
split_size <- 0.8
sample_size <- floor(split_size * nrow(mtcars))
train_indices <- sample(seq_len(nrow(mtcars)), size = sample_size)
# Splitting dataframe
train <- mtcars[train_indices, ]
test <- mtcars[-train_indices, ]
label_train <- train[, 9]
data_train_t <- train[, -9]
```

<u>**Logistic regression:**</u>

```{r}
# Logistic regression
l_model <- LogitBoost(data_train_t, label_train)
```

```{r}
train %>% ggplot() + geom_point(mapping = aes(x = mpg, y = disp)) + geom_smooth(mapping = aes(x = mpg, y = disp),method = 'glm')

```

<u>**Prediction model:**</u>

```{r}
# Prediction
pr <- predict(l_model, test, type = "raw")
l_prediction <-  data.frame(car_name = row.names(test), test$mpg, test$am, pr)
l_prediction
```

<br>

<u>**Now unsupervised learning K Means**</u>

```{r, echo=FALSE}
## Now unsupervised learning KMeans
## Fitt
clusters <- kmeans(data[,2:3], 5) # mpg , cyl # 5 Clusters
# Save the cluster number in the dataset as column 'target'
data$target <- as.factor(clusters$cluster)
plotcluster(data[,2:3], clusters$cluster ,pointsbyclvecd=FALSE)
```

<u>**The accuracy of the model**</u>

```{r}
#accuracy 
accuracy <- clusters$betweenss/clusters$totss
```
