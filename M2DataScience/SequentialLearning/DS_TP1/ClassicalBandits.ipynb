{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "StGsDuF_Whon"
   },
   "source": [
    "# Playing Stochastic Multi-Armed Bandit Games\n",
    "\n",
    "Code inspired by the [Pyma bandit library](https://www.di.ens.fr/~cappe/Code/PymaBandits/) by O. Capp√© and A. Garivier and the [SMPyBandit library](https://github.com/SMPyBandits/SMPyBandits) of Lilian Besson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xm3EFUiMWho1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "# Bandit specific functions and classes (given) \n",
    "import Arms as arm\n",
    "from StochasticBandit import *\n",
    "from BanditTools import * \n",
    "import BanditBaselines as alg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CnC90yPhWhpQ"
   },
   "source": [
    "# I) Getting started with simple strategies\n",
    "\n",
    "## 1) Creating a bandit environment \n",
    "\n",
    "Different arm classes are defined in `Arms.py` (you are welcome to add more!). All arms can be samples (i.e. produce random realization from their underlying distribution) using the \"sample\" method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm1 = arm.Bernoulli(0.7)\n",
    "print(\"The mean of arm 1 is\",arm1.mean)\n",
    "print(\"A sample from arm 1 is\",arm1.sample(),\"\\n\")\n",
    "\n",
    "\n",
    "arm2 = arm.Gaussian(1,0.5)\n",
    "print(\"The variance of arm 2 is\",arm2.variance)\n",
    "print(\"A sample from arm 2 is\",arm2.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a multi-armed bandit (a `MAB` object), you need to input a list of such arms. Some functions in `StochasticBandits.py` also define directly some particular MAB objects. MAB object have a method named generateReward that takes as an input and arm and generate a random reward from the underlying distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P1bpRGo1WhpW"
   },
   "outputs": [],
   "source": [
    "bandit1 = MAB([arm.Bernoulli(0.2), arm.Gaussian(0.9, 1), arm.TruncatedExponential(2, 1)])\n",
    "nbArms = bandit1.nbArms\n",
    "\n",
    "bandit2 = BernoulliBandit([0.3, 0.4, 0.5]) # directly creates a Bernoulli bandit from a vector of means\n",
    "\n",
    "\n",
    "print(\"The means of bandit instance 1 are\", bandit1.means)\n",
    "print(\"The means of bandit instance 2 are\", bandit2.means)\n",
    "\n",
    "print(\"Generated reward from arm 0 in bandit 2:\", bandit2.generateReward(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TQceO7tuWhpq"
   },
   "source": [
    "## 2) Playing a bandit strategy \n",
    "\n",
    "In `BanditBaselines.py` you will find the two simplest examples of bandit strategy that we saw in class: the greedy strategy (Follow The Leader), which always selects the empirical best arm, and the Uniform strategy, which always selects an arm uniformly at random. A bandit strategy is a class that has two important methods: \n",
    "* self.chooseArmToPlay() : select the next arm to play \n",
    "* self.receiveReward(a,rew) : update the strategy when arm a was drawn and the reward rew was observed\n",
    "\n",
    "I encourage you to keep the same structure for the smarter bandit strategies that you will implement in this notebook : specifying these two methods (and of course selecting the right attributes for the class). \n",
    "\n",
    "The following function can be used to run a given bandit strategy on some bandit instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mc53yKUNWhpw"
   },
   "outputs": [],
   "source": [
    "def OneBanditOneLearnerOneRun(bandit, strategy, timeHorizon):\n",
    "    \"\"\"\n",
    "    Run a bandit strategy (strategy) on a MAB instance (bandit) for (timeHorizon) time steps\n",
    "    output : sequence of arms chosen, sequence of rewards obtained\n",
    "    \"\"\"\n",
    "    selections = []\n",
    "    rewards = []\n",
    "    strategy.clear() # reset previous history\n",
    "    for t in range(timeHorizon):\n",
    "        # choose the next arm to play with the bandit algorithm\n",
    "        arm = strategy.chooseArmToPlay()\n",
    "        # get the reward of the chosen arm\n",
    "        reward = bandit.generateReward(arm)\n",
    "        # update the algorithm with the observed reward\n",
    "        strategy.receiveReward(arm, reward)\n",
    "        # store what happened\n",
    "        selections.append(arm)\n",
    "        rewards.append(reward)\n",
    "    return selections, rewards\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use to it visualize what happen during a run of a bandit strategy, first what arms get selected. Don't hesitate to re-evaluate this cell multiple times, which will perform different random runs of the algorithm. You can also try to see what happens with the uniform strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ViewSelections(selections):\n",
    "    plt.clf()\n",
    "    nbBins=max(selections) + 1\n",
    "    plt.xlabel(\"Arms\", fontsize=14)\n",
    "    plt.ylabel(\"Number of arms selections\", fontsize=14)\n",
    "    plt.hist(selections, nbBins)\n",
    "    plt.title(\"Number of selections of each arm\", fontsize=14)\n",
    "    axes=plt.gca()\n",
    "    axes.xaxis.set_ticks(range(nbBins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeHorizon = 200\n",
    "bandit = bandit2\n",
    "nbArms = bandit.nbArms\n",
    "\n",
    "strategy = alg.FTL(nbArms)\n",
    "#strategy = alg.UniformExploration(nbArms)\n",
    "\n",
    "# generating a random trajectory of the bandit algorithm\n",
    "selections, rewards = OneBanditOneLearnerOneRun(bandit, strategy, timeHorizon)\n",
    "\n",
    "# compute average reward\n",
    "print(\"the means of the bandit are\",bandit.means)\n",
    "print(\"the average reward collected is\",np.sum(rewards)/timeHorizon)\n",
    "\n",
    "# Histogram of the number of selections\n",
    "ViewSelections(selections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (expected) cumulative regret of a strategy $\\mathcal{A}=(A_t)_{t\\in \\mathbb{N}^*}$ is defined as $$\\mathcal{R}_T(\\mathcal{A}) = \\mathbb{E}\\left[\\sum_{t=1}^{T} (\\mu^\\star - \\mu_{A_t})\\right].$$\n",
    "The random quantity inside the expectation is the cumulative regret on one simulation (sometimes called pseudo-regret), ${R}_T(\\mathcal{A}) = \\sum_{t=1}^{T} (\\mu^\\star - \\mu_{A_t})$. \n",
    "\n",
    "**Write a function that returns a vector $({R}_t(\\mathcal{A}))_{t \\in [1,T]}$, which gives the cumulative (pseudo)-regret in each time step.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CumulativeRegret(bandit,selections):\n",
    "    \"\"\"Compute the pseudo-regret associated to a sequence of arm selections\"\"\"\n",
    "    T = len(selections)\n",
    "    meansB = np.array(bandit.means)\n",
    "    mustar = max(meansB)\n",
    "    return np.cumsum(mustar*np.ones(T)-meansB[selections])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ghgw0SD1WhqA"
   },
   "source": [
    "You can now visualize the regret as a function of time. Trying on different runs, you will see that the regret of FTL is either constant, or linear in $t$. Its expected regret will thus be linear, as we shall see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3KdllLxJWhqF"
   },
   "outputs": [],
   "source": [
    "bandit = bandit2\n",
    "timeHorizon = 1000\n",
    "strategy = alg.FTL(nbArms)\n",
    "selec, rewards = OneBanditOneLearnerOneRun(bandit, strategy, timeHorizon)\n",
    "regret = CumulativeRegret(bandit, selec)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.clf()\n",
    "plt.xlabel(\"Time steps\", fontsize=14)\n",
    "plt.ylabel(\"Cumulative regret\", fontsize=14)\n",
    "plt.title(\"Regret as a function of time\")\n",
    "plt.plot(range(timeHorizon), regret, 'black', linewidth=1)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EOyZy5FIWhqu"
   },
   "source": [
    "## 3) Comparison of two (bad) algorithm on multiple runs\n",
    "\n",
    "The regret is defined as an **expectation**, so we need **several runs** to estimate its value. We can also take a look at empirical distribution of the pseudo-regret. The function below gathers results accross multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "06GnW6mFWhqz"
   },
   "outputs": [],
   "source": [
    "def OneBanditOneLearnerMultipleRuns(bandit, strategy, timeHorizon, N_exp, tsave=[]):\n",
    "    \"\"\"\n",
    "    Perform N_exp runs of a bandit strategy (strategy) on a MAB instance (bandit) for (timeHorizon) time steps \n",
    "    and compute the pseudo-regret of each run \n",
    "    optional : tsave is a vector of time steps in which the results will be stored (set to 1:timeHorizon by default)\n",
    "    output : a table of size N_exp x |tsave| in which each row is the pseudo-regret at the sub-sampled times \n",
    "    \"\"\"\n",
    "    if (len(tsave) == 0):\n",
    "        tsave = np.arange(timeHorizon)\n",
    "    savedTimes = len(tsave)\n",
    "    Regret = np.zeros((N_exp, savedTimes)) # Store the regret values on different runs\n",
    "    for n in range(N_exp):\n",
    "        np.random.seed()\n",
    "        # run the bandit strategy\n",
    "        selections, rewards = OneBanditOneLearnerOneRun(bandit, strategy, timeHorizon)\n",
    "        # compute its pseudo-regret\n",
    "        regret_one_run = CumulativeRegret(bandit, selections)\n",
    "        # store (a sub-sampling of) the cumulative regret\n",
    "        Regret[n, :] = np.array(regret_one_run)[tsave] \n",
    "    return Regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = MAB([arm.Bernoulli(0.2), arm.Bernoulli(0.5), arm.Gaussian(0.6, 0.25), arm.TruncatedExponential(1, 1)])\n",
    "bandit.means\n",
    "print(bandit.means)\n",
    "\n",
    "algo1 = alg.FTL(bandit.nbArms)\n",
    "algo2 = alg.UniformExploration(bandit.nbArms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I0bXeovUWhrD"
   },
   "source": [
    "### a) For one algorithm (FTL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "95JC4ap-WhrG"
   },
   "outputs": [],
   "source": [
    "N_exp = 200\n",
    "timeHorizon = 500\n",
    "tsave = np.arange(1, timeHorizon, 10) \n",
    "#print(tsave)\n",
    "Regret = OneBanditOneLearnerMultipleRuns(bandit, algo1, timeHorizon, N_exp, tsave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gUcd8vO6WhrU"
   },
   "source": [
    "One can start by displaying the mean regret and some quantiles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jeFrpgpRWhra"
   },
   "outputs": [],
   "source": [
    "meanRegret = np.mean(Regret, 0)\n",
    "upperQuantile = np.quantile(Regret, 0.95, 0) \n",
    "lowerQuantile = np.quantile(Regret, 0.05, 0)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(tsave, meanRegret, linewidth=3.0, color=\"b\", label=\"mean regret for \"+algo1.name())\n",
    "plt.plot(tsave, upperQuantile, linestyle=\"dashed\", color=\"b\")\n",
    "plt.plot(tsave, lowerQuantile, linestyle=\"dashed\", color=\"b\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S4rliwcrWhrz"
   },
   "source": [
    "#### b) FTL versus Uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunExpes(algorithms,bandit,N_exp,timeHorizon,step=10,quantiles = \"on\"):\n",
    "    '''run experiments for multiple algorithms and display their regret on the same plot'''\n",
    "    plt.clf()\n",
    "    tsave = np.arange(1,timeHorizon,step)\n",
    "    colors = [\"b\",\"r\",\"g\",\"k\",\"m\"]\n",
    "    for i in range(len(algorithms)):\n",
    "        algo=algorithms[i]\n",
    "        Regret = OneBanditOneLearnerMultipleRuns(bandit, algo, timeHorizon, N_exp, tsave)\n",
    "        plt.plot(tsave, np.mean(Regret, 0), linewidth=2.0, color=colors[i], label=\"mean regret of \"+ algo.name())\n",
    "        if (quantiles == \"on\"):\n",
    "            plt.plot(tsave, np.quantile(Regret, 0.95, 0), tsave, np.quantile(Regret,0.05,0), linestyle=\"dashed\", color=colors[i])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p3NgdcgIWhr1"
   },
   "outputs": [],
   "source": [
    "N_exp = 200\n",
    "timeHorizon = 500\n",
    "tsave = np.arange(1, timeHorizon, 10)\n",
    "\n",
    "RunExpes([algo1,algo2],bandit,N_exp,timeHorizon,10,\"on\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Njkcs6cnWhr6"
   },
   "source": [
    "As you can see, both algorithms have **linear regret**. The regret of FTL is smaller than that of the Uniform strategy, however its pseudo-regret has a much higher variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II) Implementing UCB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCB($\\alpha$) algorithm that we saw in class selects in round $t$ the arm \n",
    "$$A_{t} = \\underset{a}{\\text{argmax}} \\left[\\hat{\\mu}_a(t-1) + \\sqrt{\\frac{\\alpha \\log(t)}{N_a(t-1)}}\\right],$$\n",
    "where $\\hat{\\mu}_a(t)$ is the empirical mean of arm $a$ after $t$ rounds and $N_a(t)$ is the number of selections of arm $a$ till that time. \n",
    "\n",
    "**Implement UCB($\\alpha$)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB:\n",
    "    \"\"\"UCB1 with parameter alpha\"\"\"\n",
    "    def __init__(self, nbArms, alpha=1/2):\n",
    "        self.nbArms = nbArms\n",
    "        self.alpha = alpha\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.nbDraws = np.zeros(self.nbArms)\n",
    "        self.cumRewards = np.zeros(self.nbArms)\n",
    "        self.t = 0\n",
    "    \n",
    "    def chooseArmToPlay(self):\n",
    "        # TO BE COMPLETED\n",
    "        pass\n",
    "\n",
    "    def receiveReward(self, arm, reward):\n",
    "        self.t = self.t + 1\n",
    "        self.nbDraws[arm] = self.nbDraws[arm] + 1\n",
    "        self.cumRewards[arm] = self.cumRewards[arm] + reward\n",
    "\n",
    "    def name(self):\n",
    "        return \"UCB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that UCB with $\\alpha = 0$ coincides with Follow the Leader and has therefore linear regret, whereas $\\alpha = 1/2$ always guarantees logarithmic regret on every bandit instance with bounded rewards. Still, on a particular instance it is possible to find a value of $\\alpha$ that leads to better performance compared to $\\alpha = 1/2$ (but it will not work on *any* problem instance). \n",
    "\n",
    "**On a bandit problem of your choice run UCB with different values of $\\alpha$ and find a good value for this instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III) Thompson Sampling for Bernoulli bandits \n",
    "\n",
    "    For Bernoulli bandit, we saw in class that UCB is not the best possible algorithm, and the goal of this part is to implement a better algorithm for Bernoulli rewards: Thompson Sampling, which is a Bayesian algorithm. \n",
    "\n",
    "Letting $\\pi_a(t)$ denote the posterior distribution on arm $a$ after $t$ rounds, Thompson Sampling can be implemented as follows: \n",
    "\n",
    "$$\\forall a, \\theta_a(t) \\sim \\pi_a(t-1), \\ \\ \\ A_{t} = \\underset{a}{\\text{argmax }} \\theta_a(t)$$\n",
    "\n",
    "In the Bernoulli case, with a uniform (or more generally a Beta$(\\alpha,\\beta)$) prior distribution on each mean, the posterior distribution remains a Beta distribution. More precisely, one has\n",
    "\n",
    "$$\\pi_a(t) = \\mathrm{Beta}\\left(\\alpha + S_a(t), \\beta + N_a(t) - S_a(t)\\right),$$\n",
    "\n",
    "where $S_a(t)$ is the sum of rewards received from arm $a$ after $t$ rounds, and $N_a(t)$ is the number of selections of that arm, as before.\n",
    "\n",
    "**Implement Thompson Sampling for Bernoulli bandits with a Beta prior.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSampling:\n",
    "    \"\"\"Thompson Sampling with Beta(a,b) prior and Bernoulli rewards\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the regret of UCB and Thompson Sampling on the following two bandit instances**\n",
    "\n",
    "In the first bandit, the means are all close to $1/2$, whereas in the second the means are all very low, which corresponds to what typically happens in applications to online advertisement. Comment on the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banditA = BernoulliBandit([0.45, 0.5, 0.6])  \n",
    "banditB = BernoulliBandit([0.1, 0.05, 0.02, 0.01])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Comparison on bandit A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0OYE5jzZWhtH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Comparison on bandit B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thompson Sampling always outperforms UCB, but the difference is much more striking when the means are low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In a bandit instance for which the rewards are now arbitrary bounded distributions supported in $[0,1]$ (e.g. truncated exponentials), can you think of an adaptation of the (binary) Thompson Sampling with the same regret guarantees?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV) Stochastic versus adversarial algorithms \n",
    "\n",
    "\n",
    "The EXP3 algorithm that we saw in class can be used to solve a stochastic bandit problem with rewards in $[0,1]$, by defining an appropriate loss function. \n",
    "\n",
    "**How do you define the loss function, and what upper bound do we have on the expected regret $R_T(\\text{EXP3})$?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement EXP3 for stochastic bandits, and compare it to Thompson Sampling. For what kind of instances could this algorithm perform best?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "RLSS_Bandits_correction.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
