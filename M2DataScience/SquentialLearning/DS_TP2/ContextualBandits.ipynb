{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "StGsDuF_Whon"
   },
   "source": [
    "# Trying Contextual Bandit Algorithms \n",
    "\n",
    "The two notebooks on bandits need to be finished and uploaded in a single zip file on this link https://nextcloud.univ-lille.fr/index.php/s/pK7qsbgqgeirbGk by **January 30th, 2023**. Please include all the files that allow to run your code and use Name_FirstName.zip as the name of the folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xm3EFUiMWho1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "# Bandit specific functions and classes (same as last time) \n",
    "import Arms as arm\n",
    "from StochasticBandit import * \n",
    "from BanditBaselines import * # you will need to add UCB alpha to your baselines \n",
    "\n",
    "from Experiments import * # all the previous functions to run experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CnC90yPhWhpQ"
   },
   "source": [
    "# I) Defining a Linear Bandit Environement \n",
    "\n",
    "For simplicity, we will first experiment in a setting in which the feature $x_a \\in \\mathbb{R}^d$ associated to each arm $a$ is fixed in every rounds. That is, the set of arms available in round $t$ is  $$\\mathcal X_t = \\{x_1,\\dots,x_K\\} \\subseteq \\mathbb R^d$$ and it can still be identified with $\\{1,\\dots,K\\}$. \n",
    "\n",
    "In this linear bandit model, when arm $A_t \\in \\{1,\\dots,K\\}$ is chosen in round $t$, a reward \n",
    "$$r_t = x_{A_t}^\\top \\theta_\\star + \\varepsilon_t$$\n",
    "is generated, for some $\\theta_\\star \\in \\mathbb R^d$ (unknown to the agent). To generate data, we will further assume that the noise is Gaussian with zero mean and some standard deviation $\\sigma^2$. \n",
    "\n",
    "The following function creates such a linear bandit model and notably depends on a matrix $X \\in \\mathbb{R}^{K \\times d}$ where each row $a$ of $X$ is $x_a^\\top$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearBandit(X,theta,sigma):\n",
    "    \"\"\"X : matrix of features of dimension (K,d), \n",
    "    theta : regression vector of dimension (d,1), \n",
    "    sigma : stdev of Gaussian noise\"\"\"\n",
    "    Arms = []\n",
    "    (K,d)=np.shape(X)\n",
    "    for k in range(K):\n",
    "        mu = np.dot(X[k,:],theta)[0]\n",
    "        arm = arms.Gaussian(mu,sigma)\n",
    "        Arms.append(arm)\n",
    "    return MAB(Arms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pick a random linear bandit instance with normalized features (i.e. $\\|x_a\\|\\leq 1$) and $\\|\\theta\\|=1$.** \n",
    "\n",
    "Display the means and the best arm and settle for a \"reasonnably interesting\" problem with a gap that is not too large between the best and second-best arm. You may wish to save some interesting pairs (X,$\\theta$) of features and regression parameters to work on later, for different dimensions / number of arms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best arm is 2\n",
      "gaps are [1.3812589  0.10524724 0.         0.45548005 1.08854924 0.88448009\n",
      " 0.4095742  1.04788709 0.94367084 1.79158172]\n"
     ]
    }
   ],
   "source": [
    "K=10\n",
    "d=5 \n",
    "\n",
    "X = np.random.normal(0,1,(K,d))\n",
    "\n",
    "for k in range(K):\n",
    "    X[k,:]=X[k,:]/np.linalg.norm(X[k,:])\n",
    "\n",
    "theta = np.random.random((d,1))\n",
    "theta = theta / np.linalg.norm(theta)\n",
    "\n",
    "sigma=0.5\n",
    "\n",
    "linear_bandit = LinearBandit(X,theta,sigma)\n",
    "\n",
    "#print(\"the means are\",linear_bandit.means)\n",
    "print(\"the best arm is\",linear_bandit.bestarm)\n",
    "print(\"gaps are\",linear_bandit.means[linear_bandit.bestarm]-linear_bandit.means)\n",
    "\n",
    "# save features and theta if you which to try this bandit problem again\n",
    "#np.savetxt(\"X1.csv\", X, delimiter=\",\")\n",
    "#np.savetxt(\"theta1.csv\", X, delimiter=\",\")\n",
    "\n",
    "# load saved features and theta\n",
    "#X = np.genfromtxt(\"X1.csv\", delimiter=\",\")\n",
    "#theta = np.genfromtxt(\"theta1.csv\", delimiter=\",\")\n",
    "#linear_bandit = LinearBandit(X,theta,sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II) Implementing least-squares and the greedy strategy\n",
    "\n",
    "The function below gives the code for the Follow-the-Leader algorithm using least-square estimation to estimate $\\theta_\\star$ and the mean reward of each arm, i.e. the algorithm selecting in round $t+1$ arm  \n",
    "$$A_{t+1} = \\underset{a \\in \\{1,\\dots,K\\}}{\\text{argmax}} \\ x_a^\\top \\hat{\\theta}_t^{\\lambda},$$\n",
    "where $\\hat{\\theta}_t^{\\lambda}$ is the regularized least-square estimate, with regularization parameter $\\lambda$.\n",
    "\n",
    "**Complete the code with the online least-square update that is required.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Greedy:\n",
    "    \"\"\"follow the leader using least-square estimation\"\"\"\n",
    "    def __init__(self,X,reg=1):\n",
    "        # the algorithms is fed with the known matrix of features (K,d) and the regularization parameter\n",
    "        self.features = X\n",
    "        (self.nbArms,self.dimension) = np.shape(X)\n",
    "        self.reg = reg\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        # initialize the design matrix, its inverse, \n",
    "        # the vector containing the sum of r_s*x_s and the least squares estimate\n",
    "        self.Design = self.reg*np.eye(self.dimension)\n",
    "        self.DesignInv = (1/self.reg)*np.eye(self.dimension)\n",
    "        self.Vector = np.zeros((self.dimension,1))\n",
    "        self.thetaLS = np.zeros((self.dimension,1)) # regularized least-squares estimate\n",
    "    \n",
    "    def chooseArmToPlay(self):\n",
    "        # compute the vector of estimated means  \n",
    "        muhat = np.dot(self.features,self.thetaLS)\n",
    "        # select the arm with largest estimated mean \n",
    "        return randmax(muhat)\n",
    "\n",
    "\n",
    "    def receiveReward(self,arm,reward):\n",
    "        x = self.features[arm,:].reshape((self.dimension,1))\n",
    "        self.Design = self.Design + np.dot(x,np.transpose(x)) \n",
    "        self.Vector = self.Vector + reward*x\n",
    "        # online update of the inverse of the design matrix\n",
    "        self.DesignInv =  ## TO BE COMPLETED\n",
    "        # update of the least squares estimate \n",
    "        self.thetaLS = ## TO BE COMPLETED\n",
    "\n",
    "    def name(self):\n",
    "        return \"greedyLB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment with the greedy strategy on one run. Is it finding the best arm?** \n",
    "\n",
    "Keep in mind that things can vary a lot between two different runs, don't hesitate to try multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "T = 1000\n",
    "strategy1 = Greedy(X)\n",
    "\n",
    "selections,rewards = OneBanditOneLearnerOneRun(linear_bandit, strategy1, T)\n",
    "    \n",
    "ViewSelections(selections)\n",
    "print(\"the best arm is\",linear_bandit.bestarm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear bandit model that we created is a particular instance of a multi-armed bandit model in which each arm is Gaussian with variance $\\sigma^2$. Therefore, we can also apply a standard UCB algorithm for this problem (that will not leverage the fact that all the means depend on the same regression parameter). With a well-chosen parameter for the UCB, this algorithm is guaranteed to have logarithmic regret on any linear bandit model (but the regret may be smaller for an algorithm leveraging the linear structure).\n",
    "\n",
    "**Compare the regret (estimated over multiple runs) of the greedy algorithm implemented above with that of a UCB algorithm and that of the Follow The Leader algorithm implemented last time (i.e. the greedy algorithm that does *not* use the linear structure).**\n",
    "\n",
    "For UCB, use the best theoretically-valid tuning and justify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha=1000 # use a more reasonnable value for alpha of course ;)\n",
    "Nexp=200\n",
    "T=2000\n",
    "\n",
    "strategy1 = Greedy(X)\n",
    "strategy2 = FTL(K)\n",
    "strategy3 = UCB(K,alpha)\n",
    "\n",
    "RunExpes([strategy1, strategy2,strategy3],linear_bandit,Nexp,T,10,\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TQceO7tuWhpq"
   },
   "source": [
    "# III) Smarter algorithms\n",
    "\n",
    "We discussed the following two algorithms in class: \n",
    "\n",
    "* LinUCB taking as an input a threshold function $\\beta(t,\\delta)$: \n",
    "$$A_{t+1}^{\\text{LinUCB}} = \\underset{a \\in \\{1,\\dots,K\\}}{\\text{argmax}} \\left[x_a^\\top \\hat{\\theta}_t^{\\lambda} + \\beta(t,\\delta) ||x_a||_{\\left(B_t^{\\lambda}\\right)^{-1}}\\right]$$\n",
    "* Thompson Sampling, taking as an input a posterior inflation parameter $v$ \n",
    "$$A_{t+1}^{\\text{LinTS}} = \\underset{a \\in \\{1,\\dots,K\\}}{\\text{argmax}} \\ x_a^\\top \\tilde{\\theta}_t$$\n",
    "where $\\tilde{\\theta}_t$ is a sample from $\\mathcal{N}\\left(\\hat{\\theta}_t^{\\lambda}, v^2 \\left(B_t^{\\lambda}\\right)^{-1}\\right)$.\n",
    "\n",
    "**In terms of computational complexity, which algorithm is preferable?**\n",
    "\n",
    "**Recall the parameters that need to be tuned for each method, and the tuning for which the two algorithms have regret guarantees.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement one of these contextual algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare two versions of this algorithms: one with the theoretical tuning seen in class, and one with a (better) heuristic tuning of your choice. Include the non-contextual UCB algorithm in your comparison. Comments on the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "RLSS_Bandits_correction.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d212184fe9e0d547803da3d7e8d0646a0cac28eb99def6e925847913b35f2386"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
