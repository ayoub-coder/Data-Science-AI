{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5JeDA6Ef9M3"
      },
      "source": [
        "### import of important libraries"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Competition** : Kaggle at Ecole Centrale Lille  <br>\n",
        "**Team**: Ayoub youssoufi, Yasser Zidani, Hatim Alouane "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwCRIwp1Pebj"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "###x = train.iloc[:, 1:2].values\n",
        "###y = train.iloc[:, 2:].values\n",
        "\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
        "\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBdVdedjcRGx"
      },
      "source": [
        "### Import DataSets & Pre Processing: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awUlyemwvU9y"
      },
      "outputs": [],
      "source": [
        "#import X_trainn \n",
        "import pandas as pd\n",
        "X_train = pd.read_csv(\"/content/X_trainn.csv\", sep=';' , header= None, encoding='latin-1',names=[\"Id\", \"Caption\"])\n",
        "X_train=X_train[1:]\n",
        "\n",
        "#import y_train\n",
        "y_train = pd.read_csv(\"y_train.csv\",sep=';', header=None,names=[\"Id\",\"Catégorie1\", \"Catégorie2\", \"Catégorie3\", \"Catégorie4\" ])\n",
        "y_train=y_train[1:]\n",
        "\n",
        "#import nonlabeled\n",
        "nonlabeled = pd.read_csv(\"nonlabeled_data.csv\",sep=';', header=None,names=[\"Id\", \"Caption\"])\n",
        "nonlabeled=nonlabeled[1:]\n",
        "\n",
        "#we have investigated that y_train isn't of a type of int \n",
        "y_train.astype(int) \n",
        "y_train.iloc[:, 2:].values.astype(int)\n",
        "\n",
        "#merge X_trainn and y_train so that we have a full dataset of inputs which are captions and 4 outputs= multi labels\n",
        "train = pd.merge(X_train, y_train, on='Id')\n",
        "#the same for Categories, Id we have found that it's not on int type\n",
        "y_train[\"Catégorie1\"].astype(int)\n",
        "y_train[\"Catégorie2\"].astype(int)\n",
        "y_train[\"Catégorie3\"].astype(int)\n",
        "y_train[\"Catégorie4\"].astype(int)\n",
        "train[\"Id\"] = train[\"Id\"].astype(int)\n",
        "train[\"Caption\"] = train[\"Caption\"].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60oJ0N192vTd"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def clean_reponses(df):\n",
        "  #initialise la liste des tokens \n",
        "  tokens = []\n",
        "  \n",
        "  #boucle sur chaque réponse\n",
        "  for reponse in df:\n",
        "    # retire les urls\n",
        "    re_url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    reponse = re_url.sub('', reponse)\n",
        "    # retire les ponctuations\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    reponse = re_punc.sub(' ', reponse)\n",
        "    # convertit en minuscules\n",
        "    reponse = reponse.lower()\n",
        "    # tokenisation\n",
        "    reponse = word_tokenize(reponse)\n",
        "    # ajoute les tokens à la liste\n",
        "    tokens.append(reponse)\n",
        "    \n",
        "  return tokens\n",
        "rep_train = clean_reponses(train['Caption'])\n",
        "rep_nonlabeled = clean_reponses(nonlabeled['Caption'])\n",
        "rep_train = clean_reponses(train['Caption'])\n",
        "rep_nonlabeled = clean_reponses(nonlabeled['Caption'])\n",
        "rep_train = clean_reponses(train['Caption'].astype(str))\n",
        "rep_nonlabeled = clean_reponses(nonlabeled['Caption'].astype(str))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4JHUOnVGCBL"
      },
      "outputs": [],
      "source": [
        "#Investigation of the length of the captions \n",
        "reponses_word_count = []\n",
        "\n",
        "for i in train['Caption']:\n",
        "  \n",
        "  if isinstance(i, str):\n",
        "        reponses_word_count.append(len(i.split()))\n",
        "  else:\n",
        "        reponses_word_count.append(0)\n",
        "\n",
        "for i in nonlabeled['Caption']:\n",
        "      reponses_word_count.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'Longueur des réponses':reponses_word_count})\n",
        "\n",
        "length_df.hist(bins = 100, range=(0,100),figsize=(8,4))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IVHC3_fp4P_"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('french'))\n",
        "\n",
        "def clean_reponses(df):\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  #retire la ponctuation\n",
        "  tokens = [re_punc.sub(' ',reponse) for reponse in df]\n",
        "  #convertit en minuscules\n",
        "  tokens = [reponse.lower() for reponse in tokens]\n",
        "  \n",
        "  #split chaque réponse en liste de mots\n",
        "  reponses = []\n",
        "  reponses = [reponse.split() for reponse in tokens]\n",
        "  \n",
        "  #deleting stop_words\n",
        "  for reponse in reponses:\n",
        "    for word in reponse:\n",
        "      if word in stop_words:\n",
        "        del reponse[reponse.index(word)]\n",
        "        \n",
        "  #deleting words of length <= 2\n",
        "  for reponse in reponses:\n",
        "    for word in reponse:\n",
        "      if len(word) <= 2:\n",
        "        del reponse[reponse.index(word)]\n",
        "  return reponses\n",
        "\n",
        "  # Let's see the words the most used: \n",
        "rep_train = clean_reponses(train['Caption'])\n",
        "rep_nonlabeled = clean_reponses(nonlabeled['Caption'])\n",
        "l=[]\n",
        "for reponse in rep_train:\n",
        "  for word in reponse:\n",
        "    l.append(word)\n",
        "\n",
        "for reponse in rep_nonlabeled:\n",
        "  for word in reponse:\n",
        "    l.append(word)\n",
        "\n",
        "\n",
        "def cleanHtml(sentence):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
        "    return cleantext\n",
        "\n",
        "\n",
        "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
        "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
        "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
        "    cleaned = cleaned.strip()\n",
        "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "def keepAlpha(sentence):\n",
        "    alpha_sent = \"\"\n",
        "    for word in sentence.split():\n",
        "        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
        "        alpha_sent += alpha_word\n",
        "        alpha_sent += \" \"\n",
        "    alpha_sent = alpha_sent.strip()\n",
        "    return alpha_sent\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Normalize the text to remove any accented characters\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
        "    # Replace any non-alphanumeric characters with a space\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    # Convert the text to lowercase\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "stemmer = SnowballStemmer(\"french\")\n",
        "def stemming(sentence):\n",
        "    stemSentence = \"\"\n",
        "    for word in sentence.split():\n",
        "        stem = stemmer.stem(word)\n",
        "        stemSentence += stem\n",
        "        stemSentence += \" \"\n",
        "    stemSentence = stemSentence.strip()\n",
        "    return stemSentence\n",
        "\n",
        "# Function to clean text of any unwanted symbols and punctuation marks\n",
        "def cleaning_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('r<.*?>', ' ', text)\n",
        "    text = re.sub(r'#\\w+', ' ', text)\n",
        "    text = re.sub(r'@\\w+', ' ', text)\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "    text = re.sub(r'http\\S+', \" \", text)\n",
        "\n",
        "    text = text.split()\n",
        "    stop_words = stopwords.words(\"english\")\n",
        "    text = \" \".join([word for word in text if not word in stop_words])\n",
        "\n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, \"\")\n",
        "\n",
        "    return text\n",
        "\n",
        "train['Caption'] = train['Caption'].apply(lambda x: cleaning_text(x))\n",
        "\n",
        "\n",
        "\n",
        "train['Caption'] = train['Caption'].str.lower()\n",
        "train['Caption'] = train['Caption'].apply(cleanHtml)\n",
        "train['Caption'] = train['Caption'].apply(cleanPunc)\n",
        "train['Caption'] = train['Caption'].apply(keepAlpha)\n",
        "train['Caption'] = train['Caption'].apply(preprocess_text)\n",
        "train['Caption'] = train['Caption'].apply(stemming)\n",
        "\n",
        "rowSums = train.iloc[:,2:].sum(axis=1)\n",
        "clean_comments_count = (rowSums==0).sum(axis=0)\n",
        "\n",
        "print(\"Total number of captions = \",len(train))\n",
        "print(\"Number of clean captions = \",clean_comments_count)\n",
        "print(\"Number of captions with labels =\",(len(train)-clean_comments_count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-uGnI0jyAD9"
      },
      "outputs": [],
      "source": [
        "#Look for the balance in data\n",
        "cat1_count = sum(train[\"Catégorie1\"].astype(int))\n",
        "cat2_count = sum(train[\"Catégorie2\"].astype(int))\n",
        "cat3_count = sum(train[\"Catégorie3\"].astype(int))\n",
        "cat4_count = sum(train[\"Catégorie4\"].astype(int))\n",
        "\n",
        "total_count = train.shape[0]\n",
        "\n",
        "# Calculate the proportion of each category\n",
        "cat1_proportion = cat1_count / total_count\n",
        "cat2_proportion = cat2_count / total_count\n",
        "cat3_proportion = cat3_count / total_count\n",
        "cat4_proportion = cat4_count / total_count\n",
        "\n",
        "print(\"Catégorie 1 proportion: {:.2f}\".format(cat1_proportion))\n",
        "print(\"Catégorie 2 proportion: {:.2f}\".format(cat2_proportion))\n",
        "print(\"Catégorie 3 proportion: {:.2f}\".format(cat3_proportion))\n",
        "print(\"Catégorie 4 proportion: {:.2f}\".format(cat4_proportion))\n",
        "\n",
        "LABEL_COLUMNS = train.columns.tolist()[2:]\n",
        "train[LABEL_COLUMNS] = train[LABEL_COLUMNS].apply(pd.to_numeric, errors='coerce')\n",
        "train[LABEL_COLUMNS].sum().sort_values().plot(kind=\"barh\");\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQkdyvW3qrgl"
      },
      "outputs": [],
      "source": [
        "#Let's stock the categories in a list that will be benfecial for us when we try to predict probabilities using softmax after\n",
        "categories = list(train.columns.values)\n",
        "categories = categories[2:]\n",
        "print(categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmD9ylDYiKB_"
      },
      "source": [
        "### **1st Try**: using the most simple model after preprocessing the data, **logistic regression** with **TfidfVectorizer** on each category among the fourth\n",
        "We will work on just the train dataset by dividing the train into trainset and testset, and we can see that for this multi label classification we did have an accuracy of about 80% for one of the four classes at least. \n",
        "Also, we started from the hypothesis of predicting labels for each category among the fourth categories, and that can be better than predicting simultaneously all the classes in the same time, because in the context of covid pandemic for a human being, we assumed that a human being is only interested in one category among the four in majority, so there is always a first important preoccupation of the person who's answer is in train[\"Caption\"], and in the other hand: we are essentially training our model to learn specific patterns and features that are associated with each of those categories. This allows my model to make more accurate predictions for each of the categories. So, we assumed that the relationships between the categories are weak or the correlations between them are not well strong\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jCI0LwZk-tg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "trainset, testset = train_test_split(train, random_state=42, test_size=0.30, shuffle=True)\n",
        "train_text = trainset['Caption']\n",
        "test_text = testset['Caption']\n",
        "train_text = trainset['Caption'].values.astype('U')\n",
        "test_text = testset['Caption'].values.astype('U')\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
        "vectorizer.fit(train_text)\n",
        "vectorizer.fit(test_text)\n",
        "\n",
        "xx_train = vectorizer.transform(train_text)\n",
        "yy_train = trainset.drop(labels = ['Id','Caption'], axis=1)\n",
        "\n",
        "\n",
        "xx_test = vectorizer.transform(test_text)\n",
        "yy_test = testset.drop(labels = ['Id','Caption'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1cgRE65l7hd"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from IPython.display import Markdown, display\n",
        "def printmd(string):\n",
        "    display(Markdown(string))\n",
        "#printmd('**bo\n",
        "\n",
        "# Using pipeline for applying logistic regression and one vs rest classifier\n",
        "LogReg_pipeline = Pipeline([\n",
        "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
        "            ])\n",
        "\n",
        "for category in categories:\n",
        "    printmd('**Processing {} captions...**'.format(category))\n",
        "    \n",
        "    # Training logistic regression model on train data\n",
        "    LogReg_pipeline.fit(xx_train, trainset[category])\n",
        "    \n",
        "    # calculating test accuracy\n",
        "    prediction = LogReg_pipeline.predict(xx_test)\n",
        "    print('Test accuracy is {}'.format(accuracy_score(testset[category], prediction)))\n",
        "    print(\"\\n\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9DLKfL_zKKy"
      },
      "source": [
        "### 2nd Try:  **EBert Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1R4t_b-rzOn2"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "from torch.utils.data import Dataset\n",
        "max_len = 32\n",
        "# Class PrepDataset to format the dataset to feed the BERT model\n",
        "class PrepDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe['Caption']\n",
        "        self.targets = self.data.Label\n",
        "        self.max_len = max_len\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFLyNwAvz7BW"
      },
      "outputs": [],
      "source": [
        "train['Label']=train[['Catégorie1', 'Catégorie2', 'Catégorie3', 'Catégorie4']].values.tolist()\n",
        "train = train.drop(columns = ['Catégorie1', 'Catégorie2', 'Catégorie3', 'Catégorie4'])\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eujUETMwzXNA"
      },
      "outputs": [],
      "source": [
        "#Split 8:2 ratio train-validation\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_size = 0.8\n",
        "train_dataset = train.sample(frac=train_size,random_state=200)\n",
        "valid_dataset = train.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "training_set = PrepDataset(train_dataset, tokenizer, max_len)\n",
        "validation_set = PrepDataset(valid_dataset, tokenizer, max_len)\n",
        "training_loader = DataLoader(training_set, batch_size=30)\n",
        "validation_loader = DataLoader(validation_set, batch_size=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw962KejzofR"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel\n",
        "class BERT(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "        self.l1 = BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
        "        self.l2 = torch.nn.Dropout(0.3)\n",
        "        self.l3 = torch.nn.Linear(768, 4)\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
        "        output_2 = self.l2(output_1)\n",
        "        output = self.l3(output_2)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5V-w3RL0bSn"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Verify the use of GPU in google colab\n",
        "device_name = tf.test.gpu_device_name()\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymyiRtu70dGl"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "epochs = 7\n",
        "learning_rate = 2e-10\n",
        "model = BERT()\n",
        "model.to(device)\n",
        "len_trainloader = len(training_loader)\n",
        "loss_f = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "train_loss = 0\n",
        "valid_loss = 0\n",
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "for epoch in range(epochs):\n",
        "  avg_loss = 0\n",
        "  items = 0\n",
        "  for batch_idx, data in enumerate(training_loader):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
        "  print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KutQxDR88ujC"
      },
      "outputs": [],
      "source": [
        "val_targets=[]\n",
        "val_outputs=[]\n",
        "valid_loss=0\n",
        "from sklearn import metrics\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch_idx, data in enumerate(validation_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
        "        val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "        val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "\n",
        "\n",
        "val_preds = (np.array(val_outputs) > 0.3).astype(int)\n",
        "accuracy = metrics.accuracy_score(val_targets, val_preds)\n",
        "f1 = metrics.f1_score(val_targets, val_preds, average='micro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tOpzumq4-Ts"
      },
      "source": [
        "# **Use of Camembert, the best approach (highest score)**\n",
        "\n",
        "Predicting labels for each category among the fourth (or only the first three) can be better than doing a prediction on all labels in the same time because:\n",
        "\n",
        "Better accuracy: The first three categories are well-defined and mutually exclusive, so predicting labels for each category would give us a better accuracy than predicting all the labels at the same time or on the fourth label which is a combination of all labels.\n",
        "\n",
        "Better insights: Predicting labels for each category can give us better insights into the specific **(very Real, most important)** concerns of the respondents. This can help in developing better strategies for addressing these concerns.\n",
        "\n",
        "Better decision-making: Predicting labels for each category can help decision-makers to prioritize their actions based on the most pressing concerns. This can help in making better decisions on how to allocate resources to address the concerns of the respondents.\n",
        "\n",
        "Therefore, predicting labels for each category can be a more efficient and effective way to analyze the dataset and understand the concerns of the respondentS;\n",
        "\n",
        "It's worth to say, that we trained the model separately in terms of each label based on the assumption that we developed before, and we were playing on hyperparameters of our model in each category so that we achieve the highest scores, and in the same time we didnt use non labeled data but we keeped an eye on the difference between train loss and validation loss so that we are sure that the model isn't overfitting, and then we concatenate the prediction of probabilities for the 4 categories to have our submission file "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vMc3aot37O6"
      },
      "source": [
        "#### Category_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrNgsTc3tBxm"
      },
      "outputs": [],
      "source": [
        "# Reimporte the X_train, y_train because we had to restart kernel many times to update tokenizer (else it outout a nontype error)\n",
        "X_train = pd.read_excel(\"X_train1.xlsx\",header=None,names=[\"Id\", \"Caption\"])[1:]\n",
        "X_train[\"Id\"] = X_train[\"Id\"].astype(int)\n",
        "y_train = pd.read_csv(\"y_train.csv\",sep=';',header=None,names=[\"Id\",\"category_1\",\"category_2\",\"category_3\",\"category_4\"])[1:]\n",
        "y_train=y_train.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u28ukxo-s5kE"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0pGEN-ytues"
      },
      "outputs": [],
      "source": [
        "# Importing standard libraries for every machine/deep learning pipeline\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm, trange\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Importing specific libraries for data prerpcessing, model archtecture choice, training and evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler \n",
        "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
        "from transformers import AdamW\n",
        "# import torch.optim as optim\n",
        "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "# import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_l5Uq069tzdT"
      },
      "outputs": [],
      "source": [
        "epochs = 4\n",
        "MAX_LEN = 64\n",
        "batch_size = 32\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrUjrpA6t1xB"
      },
      "outputs": [],
      "source": [
        "!pip install datsets transformers[sentencepiece]\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9FlIdWft3ei"
      },
      "outputs": [],
      "source": [
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base',do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PYKMkKht9rs"
      },
      "outputs": [],
      "source": [
        "text = X_train['Caption'].to_list()\n",
        "labels = y_train['category_1'].to_list()\n",
        "\n",
        "for i in range(len(text)):\n",
        "  if pd.isna(text[i]):\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kj7m028suM8b"
      },
      "outputs": [],
      "source": [
        "text[88]='aucune'\n",
        "text[240]=\"#NOM?\"\n",
        "text[333]=\"#NOM?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvrPJhvPupF7"
      },
      "outputs": [],
      "source": [
        "input_ids  = [tokenizer.encode(sent,add_special_tokens=True,max_length=MAX_LEN) for sent in text]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "attention_masks = []\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]  \n",
        "    attention_masks.append(seq_mask)\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,random_state=42, test_size=0.1)\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWF4xktLuvUt"
      },
      "outputs": [],
      "source": [
        "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=2)\n",
        "model.to(device) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rXRIjLnuwtz"
      },
      "outputs": [],
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=10e-8)\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "train_loss_set = []\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for _ in trange(epochs, desc=\"Epoch\"):  \n",
        "    # Tracking variables for training\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  \n",
        "    # Train the model\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Add batch to device CPU or GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # Clear out the gradients (by default they accumulate)\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        # Get loss value\n",
        "        loss = outputs[0]\n",
        "        # Add it to train loss list\n",
        "        train_loss_set.append(loss.item())    \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        # Update parameters and take a step using the computed gradient\n",
        "        optimizer.step()\n",
        "    \n",
        "        # Update tracking variables\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "\n",
        "    # Tracking variables for validation\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    # Validation of the model\n",
        "    model.eval()\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        # Add batch to device CPU or GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions\n",
        "            outputs =  model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "            loss, logits = outputs[:2]\n",
        "    \n",
        "        # Move logits and labels to CPU if GPU is used\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    \n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6YTsG8IKPp5"
      },
      "outputs": [],
      "source": [
        "## Call X_test:\n",
        "X_test = pd.read_excel(\"X_test11.xlsx\",header=None,names=[\"Id\", \"Caption\"])[1:]\n",
        "X_test[\"Id\"] = X_test[\"Id\"].astype(int)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VwjVSOTLazc"
      },
      "outputs": [],
      "source": [
        "texttest = X_test['Caption'].to_list()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSXQLkXSMTLc"
      },
      "outputs": [],
      "source": [
        "for i in range(len(texttest)):\n",
        "  if pd.isna(texttest[i]):\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssJXx4CKMQ-K"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "texttest[73]=\"#NOM?\"\n",
        "texttest[122]=\"#NOM?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2eEARyVMVHc"
      },
      "outputs": [],
      "source": [
        "# Encode the comments\n",
        "tokenized_comments_ids = [tokenizer.encode(comment,add_special_tokens=True,max_length=MAX_LEN) for comment in texttest]\n",
        "# Pad the resulted encoded comments\n",
        "tokenized_comments_ids = pad_sequences(tokenized_comments_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks \n",
        "attention_masks = []\n",
        "for seq in tokenized_comments_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "\n",
        "prediction_inputs = torch.tensor(tokenized_comments_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVEg_r20Mbdy"
      },
      "outputs": [],
      "source": [
        "# Apply the finetuned model (Camembert)\n",
        "flat_pred = []\n",
        "with torch.no_grad():\n",
        "    # Forward pass, calculate logit predictions\n",
        "    outputs =  model(prediction_inputs.to(device),token_type_ids=None, attention_mask=prediction_masks.to(device))\n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy() \n",
        "    flat_pred.extend(np.argmax(logits, axis=1).flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKw6CcmDMr-5"
      },
      "outputs": [],
      "source": [
        "flat_pred=pd.DataFrame(flat_pred)\n",
        "flat_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3uKnpxTMvPQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "flat_pred.to_csv('cat1.csv',index=False)\n",
        "files.download('cat1.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSy3Lb1f4Fl4"
      },
      "source": [
        "#### Category2: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCjtmhcv4l8Y"
      },
      "outputs": [],
      "source": [
        "text = X_train['Caption'].to_list()\n",
        "labels = y_train['category_2'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipbhNhFF4H4C"
      },
      "outputs": [],
      "source": [
        "for i in range(len(text)):\n",
        "  if pd.isna(text[i]):\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-jfdNnc4nag"
      },
      "outputs": [],
      "source": [
        "text[88]='aucune'\n",
        "text[240]=\"#NOM?\"\n",
        "text[333]=\"#NOM?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5T3IQJS4ryc"
      },
      "outputs": [],
      "source": [
        "epochs = 9\n",
        "MAX_LEN = 128\n",
        "batch_size = 16\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jpZ_EFy4R2I"
      },
      "outputs": [],
      "source": [
        "input_ids  = [tokenizer.encode(sent,add_special_tokens=True,max_length=MAX_LEN) for sent in text]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "attention_masks = []\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]  \n",
        "    attention_masks.append(seq_mask)\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,random_state=42, test_size=0.1)\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=2)\n",
        "model.to(device) \n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-6, eps=10e-8)\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMAPs-1p4f3i"
      },
      "outputs": [],
      "source": [
        "train_loss_set = []\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for _ in trange(epochs, desc=\"Epoch\"):  \n",
        "    # Tracking variables for training\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  \n",
        "    # Train the model\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Add batch to device CPU or GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # Clear out the gradients (by default they accumulate)\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        # Get loss value\n",
        "        loss = outputs[0]\n",
        "        # Add it to train loss list\n",
        "        train_loss_set.append(loss.item())    \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        # Update parameters and take a step using the computed gradient\n",
        "        optimizer.step()\n",
        "    \n",
        "        # Update tracking variables\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "\n",
        "    # Tracking variables for validation\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    # Validation of the model\n",
        "    model.eval()\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        # Add batch to device CPU or GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions\n",
        "            outputs =  model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "            loss, logits = outputs[:2]\n",
        "    \n",
        "        # Move logits and labels to CPU if GPU is used\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    \n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-pDH9qi4v_X"
      },
      "outputs": [],
      "source": [
        "X_test = pd.read_excel(\"X_test11.xlsx\",header=None,names=[\"Id\", \"Caption\"])[1:]\n",
        "X_test[\"Id\"] = X_test[\"Id\"].astype(int)\n",
        "texttest = X_test['Caption'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpQ1vJlN6xX3"
      },
      "outputs": [],
      "source": [
        "for i in range(len(texttest)):\n",
        "  if pd.isna(texttest[i]):\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KR_6bCgf602D"
      },
      "outputs": [],
      "source": [
        "texttest[73]='#NOM?'\n",
        "texttest[122]='#NOM?'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-BG324262EG"
      },
      "outputs": [],
      "source": [
        "tokenized_comments_ids = [tokenizer.encode(comment,add_special_tokens=True,max_length=MAX_LEN) for comment in texttest]\n",
        "# Pad the resulted encoded comments\n",
        "tokenized_comments_ids = pad_sequences(tokenized_comments_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks \n",
        "attention_masks = []\n",
        "for seq in tokenized_comments_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "\n",
        "prediction_inputs = torch.tensor(tokenized_comments_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiWLrK9Q63M-"
      },
      "outputs": [],
      "source": [
        "flat_pred = []\n",
        "with torch.no_grad():\n",
        "    # Forward pass, calculate logit predictions\n",
        "    outputs =  model(prediction_inputs.to(device),token_type_ids=None, attention_mask=prediction_masks.to(device))\n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy() \n",
        "    flat_pred.extend(np.argmax(logits, axis=1).flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6eTUJ2Y64-H"
      },
      "outputs": [],
      "source": [
        "cat3=np.array([1/(1+np.exp(-logits[i][1])) for i in range(len(logits))])\n",
        "cat2=np.array([1/(1+np.exp(-logits[i][1])) for i in range(len(logits))])\n",
        "cat1=np.array([1/(1+np.exp(-logits[i][1])) for i in range(len(logits))])\n",
        "cat4=np.array([1/(1+np.exp(-logits[i][1])) for i in range(len(logits))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxyRMFEm7D1x"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "new = pd.read_csv(\"/content/newprob2.csv\",sep=',',header=None,names=[\"Id\",\"category_1\",\"category_2\",\"category_3\",\"category_4\"])[1:]\n",
        "new=new.astype(float)\n",
        "new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPk0lbnv7E4l"
      },
      "outputs": [],
      "source": [
        "for i in range(157):\n",
        "  new.iloc[i][2]=cat2[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRTdHKrn7G3V"
      },
      "outputs": [],
      "source": [
        "new['Id']=new['Id'].astype(int)\n",
        "new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7eXYapC7KwI"
      },
      "outputs": [],
      "source": [
        "nb_new1=sum(new[\"category_1\"].astype(int))\n",
        "nb_new2=sum(new[\"category_2\"].astype(int))\n",
        "nb_new3=sum(new[\"category_3\"].astype(int))\n",
        "nb_new4=sum(new[\"category_4\"].astype(int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBVqJ4qv7ODc"
      },
      "outputs": [],
      "source": [
        "y_train = pd.read_csv(\"y_train.csv\",sep=';',header=None,names=[\"Id\",\"category_1\",\"category_2\",\"category_3\",\"category_4\"])[1:]\n",
        "y_train = y_train.astype(int)\n",
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrShnzcd7cP8"
      },
      "outputs": [],
      "source": [
        "nb_cat1=sum(y_train[\"category_1\"].astype(int))\n",
        "nb_cat2=sum(y_train[\"category_2\"].astype(int))\n",
        "nb_cat3=sum(y_train[\"category_3\"].astype(int))\n",
        "nb_cat4=sum(y_train[\"category_4\"].astype(int))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvyHytss8mzx"
      },
      "source": [
        "#### Interpretation: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKXALQjU_eXq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "new = pd.read_csv(\"newprob2.csv\",sep=',',header=None,names=[\"Id\",\"category_1\",\"category_2\",\"category_3\",\"category_4\"])[1:]\n",
        "new=new.astype(float)\n",
        "new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TakQpw6B_eXq"
      },
      "outputs": [],
      "source": [
        "prop4=0\n",
        "for i in range(len(new)):\n",
        "    if new.iloc[i][4]>0.9:\n",
        "        prop4=prop4+1\n",
        "prop4/157"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaWal4Hx_14v"
      },
      "outputs": [],
      "source": [
        "prop3=0\n",
        "for i in range(len(new)):\n",
        "    if new.iloc[i][3]>0.9:\n",
        "        prop3=prop3+1\n",
        "prop3/157"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veJZgnkU_794"
      },
      "outputs": [],
      "source": [
        "prop1=0\n",
        "for i in range(len(new)):\n",
        "    if new.iloc[i][1]>0.9:\n",
        "        prop1=prop1+1\n",
        "prop1/157"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVf6NQkhABfP"
      },
      "outputs": [],
      "source": [
        "prop2=0\n",
        "for i in range(len(new)):\n",
        "    if new.iloc[i][2]>0.7:\n",
        "        prop2=prop2+1\n",
        "prop2/157"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "R5JeDA6Ef9M3",
        "SBdVdedjcRGx",
        "EmD9ylDYiKB_",
        "n9DLKfL_zKKy"
      ],
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "lab1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "68eaf4da628b37f3383b4454e611c1505700ab88676e028f0decd5ce65c10e7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
