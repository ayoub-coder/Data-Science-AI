{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch distributed tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VdbsS6fRT7c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "702b4435-de6c-4256-d647-978217a9bfe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.10.0+cu111\n",
            "11.1\n",
            "8005\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)\n",
        "print(torch.backends.cudnn.version())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU properties"
      ],
      "metadata": {
        "id": "lNcP4gXhW-Am"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Activate GPU usage, Runtime -> Change Runtime Type -> Choose GPU type\n",
        "! nvidia-smi"
      ],
      "metadata": {
        "id": "ySYR-2n2XITp",
        "outputId": "b5306d67-60d7-4a5e-fdac-e81041276e11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar 15 15:03:14 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P8    33W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "oOMsdTVQXJfA",
        "outputId": "9ff89b1f-52e4-4217-b61c-347ef51c8a53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.device_count())"
      ],
      "metadata": {
        "id": "eYxcTNpOXbMp",
        "outputId": "9416ec6f-5fbc-419b-b990-6ca70512bb68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.current_device())"
      ],
      "metadata": {
        "id": "Xj-oZl--Xe1x",
        "outputId": "f003e40b-a057-477f-8942-27f678aa9170",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization by torch.distributed.init_process_group()"
      ],
      "metadata": {
        "id": "npBMwhEAUNKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.distributed as dist\n",
        "from torch.multiprocessing import Process\n",
        "\n",
        "\n",
        "def print_rank():\n",
        "    print('Hello from process {} (out of {})!'.format(dist.get_rank(), dist.get_world_size()))\n",
        "\n",
        "def init_process(rank, size, fn, backend='gloo'):\n",
        "    \"\"\" Initialize the distributed environment. \"\"\"\n",
        "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
        "    os.environ['MASTER_PORT'] = '20951'\n",
        "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
        "    fn()\n",
        "\n",
        "def main(fn, size=4):\n",
        "    processes = []\n",
        "    for rank in range(size):\n",
        "        p = Process(target=init_process, args=(rank, size, fn))\n",
        "        p.start()\n",
        "        processes.append(p)\n",
        "\n",
        "    for p in processes:\n",
        "        p.join()\n",
        "\n",
        "\n",
        "main(print_rank, size=4)"
      ],
      "metadata": {
        "id": "tuiqhGDZUN1d",
        "outputId": "9595e1f5-fa83-4c26-b7bd-185fea4fd05b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from process 1 (out of 4)!\n",
            "Hello from process 2 (out of 4)!\n",
            "Hello from process 0 (out of 4)!\n",
            "Hello from process 3 (out of 4)!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: Which method is used to launch multiple processes?  \n",
        "Q2: After initilization, the rank of the process and the worldsize can be obtained by which functions in torch.distributed?"
      ],
      "metadata": {
        "id": "Ky9itE6vUxfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Communication: broadcast"
      ],
      "metadata": {
        "id": "7xzJ4l8BVGYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def broadcast():\n",
        "    \n",
        "    rank = dist.get_rank()\n",
        "    size = dist.get_world_size()\n",
        "    tensor = torch.tensor(rank)\n",
        "    group = dist.new_group([0,1,2,3])\n",
        "    #print(f\"I am {rank} of {size} with a tensor {tensor}\")\n",
        "    \n",
        "    if rank == 0 : print(\"**********\\nStarting Communication\\n************\")\n",
        "    dist.broadcast(tensor=tensor, src=0, group=group)\n",
        "    print('Rank ', rank, ' has data ', tensor)\n",
        "\n",
        "main(broadcast, size=4)"
      ],
      "metadata": {
        "id": "Ar2QfftcU3aK",
        "outputId": "9b1ecc55-ed28-4740-942a-371e357de89b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********\n",
            "Starting Communication\n",
            "************\n",
            "Rank  3  has data  tensor(0)\n",
            "Rank  1  has data  tensor(0)\n",
            "Rank  2  has data  tensor(0)\n",
            "Rank  0  has data  tensor(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3: In the above code, which rank is the one who broadcasts?\n",
        "<br>\n",
        "Task 1: If Rank 0 just wants to broadcast to a random subset of all the processes, please write down the new code to acheive that."
      ],
      "metadata": {
        "id": "nFMyzvRnVYEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer for Task 1\n",
        "import random\n",
        "\n",
        "def broadcast_random(seed=1234):\n",
        "    rank = dist.get_rank()\n",
        "    size = dist.get_world_size()\n",
        "    tensor = torch.tensor(rank)\n",
        "    #print(f\"I am {rank} of {size} with a tensor {tensor}\")\n",
        "    \n",
        "    random.seed(seed)\n",
        "    random_group = random.sample([i for i in range(1,size)], 2)\n",
        "    random_group = random_group + [0]\n",
        "    print(f\"Rank 0 broadcasts to the group {random_group}\")\n",
        "    group = dist.new_group(random_group)\n",
        "\n",
        "    if rank == 0 : print(\"**********\\nStarting Communication\\n************\")\n",
        "    dist.broadcast(tensor=tensor, src=0, group=group)\n",
        "    print('Rank ', rank, ' has data ', tensor)\n",
        "\n",
        "main(broadcast_random, size=4)"
      ],
      "metadata": {
        "id": "Dct9a9x9VYjj",
        "outputId": "5fc84256-0364-465e-afd8-6f90f57d55f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank 0 broadcasts to the group [2, 1, 0]\n",
            "Rank 0 broadcasts to the group [2, 1, 0]\n",
            "Rank 0 broadcasts to the group [2, 1, 0]\n",
            "Rank 0 broadcasts to the group [2, 1, 0]\n",
            "**********\n",
            "Starting Communication\n",
            "************\n",
            "Rank  3  has data  tensor(3)\n",
            "Rank  2  has data  tensor(0)\n",
            "Rank  0  has data  tensor(0)\n",
            "Rank  1  has data  tensor(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Communication: reduce"
      ],
      "metadata": {
        "id": "X3XSReuNVize"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce():\n",
        "    rank = dist.get_rank()\n",
        "    size = dist.get_world_size()\n",
        "    tensor = torch.tensor(rank+1)\n",
        "    if rank == 0:\n",
        "        tensor_old = tensor.clone()\n",
        "    group = dist.new_group([0,1,2,3])\n",
        "    print(f\"I am {rank} of {size} with a tensor {tensor}\")\n",
        "    if rank == 0:\n",
        "        print(\"**********\\nStarting Communication\\n************\")\n",
        "    dist.reduce(tensor=tensor, dst=0, op=dist.ReduceOp.SUM, group = group)\n",
        "    if rank == 0:\n",
        "        tensor -= tensor_old\n",
        "    print('Rank ', rank, ' has data ', tensor.item())\n",
        "\n",
        "main(reduce, size=4)"
      ],
      "metadata": {
        "id": "hgGZWhSQVpbC",
        "outputId": "070c2983-5de8-430a-860a-e5588b1468fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am 2 of 4 with a tensor 3\n",
            "I am 0 of 4 with a tensor 1\n",
            "**********\n",
            "Starting Communication\n",
            "************I am 3 of 4 with a tensor 4\n",
            "I am 1 of 4 with a tensor 2\n",
            "Rank  3  has data  4\n",
            "Rank  2  has data  7\n",
            "\n",
            "Rank  1  has data  9\n",
            "Rank  0  has data  9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4: What does the above code acheive?\n",
        "<br>\n",
        "Q5: Check the values of every rank after \"reduce\", try to explain the reason.\n",
        "<br>\n",
        "\n",
        "Task 2 [Server-Client communication]: Write a function which runs for 10 iterations: Among each iteration, \n",
        "- rank 0 broadcasts to a random subset of all the processes, \n",
        "- the processes in the subset update their states by adding one unit, \n",
        "- rank 0 gets the average of the states from the processes in the subset."
      ],
      "metadata": {
        "id": "_xK_TR68V0Pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer for Task 2\n",
        "\n",
        "def server_client_communication(group_size=2):\n",
        "    rank = dist.get_rank()\n",
        "    size = dist.get_world_size()\n",
        "    tensor = torch.tensor(float(rank))\n",
        "    iterations = 10\n",
        "    random.seed(0)\n",
        "    seeds = [random.randint(0,10000) for i in range(iterations)]\n",
        "    for i, sd in zip(range(iterations), seeds):\n",
        "        # Step 1\n",
        "        random.seed(sd)\n",
        "        random_group = random.sample([i for i in range(1,size)], group_size)\n",
        "        random_group = random_group + [0]\n",
        "        if rank == 0: print(f\"Iter {i}: Rank 0 broadcasts to the group {random_group}\")\n",
        "        random_group_dist = dist.new_group(random_group)\n",
        "        dist.broadcast(tensor=tensor, src=0, group=random_group_dist)\n",
        "        \n",
        "        # Step 2\n",
        "        if rank in random_group and rank != 0: \n",
        "            tensor += 1\n",
        "\n",
        "        # Step 3\n",
        "        if rank == 0: tensor_old = tensor.clone()\n",
        "        dist.reduce(tensor=tensor, dst=0, op=dist.ReduceOp.SUM, group=random_group_dist)\n",
        "        if rank == 0:\n",
        "            tensor -= tensor_old\n",
        "            tensor = tensor/group_size\n",
        "\n",
        "    if rank == 0: print(f\"The final value of Rank {0} is {tensor}\")\n",
        "\n",
        "main(server_client_communication, size=4)"
      ],
      "metadata": {
        "id": "XLEtkl2YV4GY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Communication: send and receive"
      ],
      "metadata": {
        "id": "3soH0YwJWLg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def send_receive():\n",
        "    rank = dist.get_rank()\n",
        "    size = dist.get_world_size()\n",
        "    tensor = torch.tensor(rank+1)\n",
        "    print(f\"I am {rank} of {size} with a tensor {tensor}\")\n",
        "    if rank == 0:\n",
        "        print(\"**********\\nStarting Communication\\n************\")\n",
        "        dist.recv(tensor, src=1)\n",
        "    if rank == 1:\n",
        "        dist.send(tensor, dst=0)\n",
        "    if rank == 2:\n",
        "        dist.recv(tensor)\n",
        "    if rank == 3:\n",
        "        dist.send(tensor, dst=2)\n",
        "    print('Rank ', rank, ' has data ', tensor.item())\n",
        "\n",
        "main(send_receive, size=4)"
      ],
      "metadata": {
        "id": "fTx0fHoKWPGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## torch.distributed.launch()"
      ],
      "metadata": {
        "id": "4oi1pyrxWfzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Launch.py\n",
        "import os\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import argparse\n",
        "\n",
        "\n",
        "def parse():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--func', type=str, help='choose the function to execute')\n",
        "    parser.add_argument('--backend', type=str, help='choose the backend')\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "def print_rank():\n",
        "    print('Hello from process {} (out of {})!'.format(dist.get_rank(), dist.get_world_size()))\n",
        "\n",
        "def broadcast():\n",
        "    rank = dist.get_rank()\n",
        "    size = dist.get_world_size()\n",
        "    if 'OMP_NUM_THREADS' not in os.environ:\n",
        "        current_env[\"OMP_NUM_THREADS\"] = 1\n",
        "    if torch.cuda.is_available() == True:\n",
        "        device = torch.device('cuda:'+str(rank))\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "    tensor = torch.tensor(rank, device=device)\n",
        "    group = dist.new_group([0,1])\n",
        "    #print(f\"I am {rank} of {size} with a tensor {tensor.item()}\")\n",
        "    if rank == 0 : print(\"**********\\nStarting Communication\\n************\")\n",
        "    dist.broadcast(tensor=tensor, src=0, group=group)\n",
        "    print('Rank ', rank, ' has data ', tensor)\n",
        "\n",
        "\n",
        "if __name__== '__main__':\n",
        "    args = parse()\n",
        "    backend = args.backend\n",
        "    if torch.cuda.is_available() == True:\n",
        "        size = int(os.environ['WORLD_SIZE'])\n",
        "        # if torch.cuda.device_count()<size:\n",
        "            # raise ValueError('size should not larger than the number of GPUs')\n",
        "    rank = int(os.environ[\"LOCAL_RANK\"])\n",
        "    function_mapping = {'print_rank': print_rank, 'broadcast': broadcast}\n",
        "    dist.init_process_group(backend)\n",
        "    function_mapping[args.func]()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anlCvG5VhXZU",
        "outputId": "8be30228-6d77-4e64-d38f-0b8b552002eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Launch.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful detail: \n",
        "# https://github.com/pytorch/pytorch/blob/master/torch/distributed/launch.py\n",
        "!OMP_NUM_THREADS=1 torchrun --nproc_per_node=2 Launch.py --func \"print_rank\" --backend gloo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYaDYfZchcn5",
        "outputId": "bbb25ae7-13a4-4944-acdf-0c67e62afd56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from process 0 (out of 2)!\n",
            "Hello from process 1 (out of 2)!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6: Which package is used for launching multiple processes in torch.distributed.launch? [check the source code in the detail link]\n",
        "<br>\n",
        "Task 3: Reserve two GPUs from NEF and try to run the script Launch.py."
      ],
      "metadata": {
        "id": "g0cliqJhjXgr"
      }
    }
  ]
}